# PRTS: PRe-Training by Stacking

official implementation for paper **Stacking Your Transformers: An Empirical Study of Model Growth for Efficient LLMs Pre-Training** 
